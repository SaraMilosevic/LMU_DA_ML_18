{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Higgs Challenge Example using Neural Networks\n",
    "In this part we will look at the **[Higgs Boson ML Challenge](https://www.kaggle.com/c/Higgs-boson)** on Kaggle and attempt a solution using neural networks (NN). The data is available from **[CERN Open Data](http://opendata.cern.ch/record/328)**. More information about the data is available from the links, and in particular at **[Documentation](http://opendata.cern.ch/record/329/files/atlas-higgs-challenge-2014.pdf)**. The general idea is that we want to extract $H\\to\\tau\\tau$ signal from background. In particular, the selection requires one of the taus to decay into an electron or muon and two neutrinos, and the other into hadrons and a neutrino. The challenge is based on Monte Carlo events processed through the **[ATLAS detector](http://atlas.cern/)** simulation and reconstruction.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background on Neural Networks\n",
    "(based on lectures from **[ML Course on Coursera](https://www.coursera.org/learn/machine-learning)**)\n",
    "\n",
    "As we saw from the logistic regression yesterday, linear classifiers are often not the best at solving complicated problems. Neural networks introduce nonlinearity. They were originally designed to mimic the brain, and were popular in the 80s and early 90s. Recently they have become popular again, especially as deep neural networks DNNs, including convolutional NNs (CNN), recurrent NNs (RNN), etc. Those are beyond the scope of this class, but we will introduce the basics of NNs.\n",
    "\n",
    "Below is a diagram of a simple NN:\n",
    "![NNFig](https://upload.wikimedia.org/wikipedia/commons/4/46/Colored_neural_network.svg)\n",
    "It is made up of \"neurons\" that get a number of inputs, processes them, and sends the output to other neurons. Mathematically, one can represent the a neuron's \"activation\" $a = g\\left(\\theta^Tx\\right)$, where $x$ are the inputs (a vector), and $\\theta$ are the parameters (weights) of the model (also a vector), and $g$ is the activation fuction. For example, if we use a logistic function as the activation function, we can have $g\\left(\\theta^Tx\\right) = \\frac{1}{1+\\mathrm{exp}\\left(-\\theta^Tx\\right)}$, or if a Rectified Linear Unit (ReLU), $g\\left(\\theta^Tx\\right) = \\mathrm{max}\\left(0, \\theta^Tx\\right)$. The NN above has an input layer (layer 1), a hidden layer (layer 2), and an output layer (layer 3). One can have more hidden layers. Let's label the activations of layer 2 as $a_i^{(2)} = g\\left(\\theta_i^{(1)T}x\\right)$, where $i$ is the index of the individual neurons. Note that the superscript of the $\\theta$ is (1). That is because these are the weights going from layer 1 to 2. Putting together all the individual weight vectors together forms a matrix $\\Theta^{(1)}$.\n",
    "\n",
    "Using matrix notation, we can define $z^{(j)} = \\Theta^{(j-1)}a^{(j-1)}$ and then $a^{(j)} = g(z^{(j)})$. Thus evaluating the NN is a series of matrix multiplications followed by activation functions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cost function of a NN is similar to what we have for logistic regression, modified to take into account possible multiple outputs, and with more complicated regularization. In order to train the NN, we have to determine the weight matrix $\\Theta$ that minimizes the cost function. Backpropagation is the method used to do that. It calculates the partial derivatives \"errors\" for each $z_i^{(j)}$ by propagating the errors backwards. Usually something like (stochastic) gradient descent is used to solve the problem. For more details on backprorpagation, look, for example, at the **[ML course](https://www.coursera.org/learn/machine-learning)** mentioned above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's start trying to apply a NN to the Higgs Challenge data. We will start using Scikit Learn, and then try **[Keras](https://keras.io/)**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the usual setup: \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load training data\n",
    "df = pd.read_csv('data/atlas-higgs-challenge-2014-v2.csv.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EventId</th>\n",
       "      <th>DER_mass_MMC</th>\n",
       "      <th>DER_mass_transverse_met_lep</th>\n",
       "      <th>DER_mass_vis</th>\n",
       "      <th>DER_pt_h</th>\n",
       "      <th>DER_deltaeta_jet_jet</th>\n",
       "      <th>DER_mass_jet_jet</th>\n",
       "      <th>DER_prodeta_jet_jet</th>\n",
       "      <th>DER_deltar_tau_lep</th>\n",
       "      <th>DER_pt_tot</th>\n",
       "      <th>...</th>\n",
       "      <th>PRI_jet_leading_eta</th>\n",
       "      <th>PRI_jet_leading_phi</th>\n",
       "      <th>PRI_jet_subleading_pt</th>\n",
       "      <th>PRI_jet_subleading_eta</th>\n",
       "      <th>PRI_jet_subleading_phi</th>\n",
       "      <th>PRI_jet_all_pt</th>\n",
       "      <th>Weight</th>\n",
       "      <th>Label</th>\n",
       "      <th>KaggleSet</th>\n",
       "      <th>KaggleWeight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100000</td>\n",
       "      <td>138.470</td>\n",
       "      <td>51.655</td>\n",
       "      <td>97.827</td>\n",
       "      <td>27.980</td>\n",
       "      <td>0.91</td>\n",
       "      <td>124.711</td>\n",
       "      <td>2.666</td>\n",
       "      <td>3.064</td>\n",
       "      <td>41.928</td>\n",
       "      <td>...</td>\n",
       "      <td>2.150</td>\n",
       "      <td>0.444</td>\n",
       "      <td>46.062</td>\n",
       "      <td>1.24</td>\n",
       "      <td>-2.475</td>\n",
       "      <td>113.497</td>\n",
       "      <td>0.000814</td>\n",
       "      <td>s</td>\n",
       "      <td>t</td>\n",
       "      <td>0.002653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100001</td>\n",
       "      <td>160.937</td>\n",
       "      <td>68.768</td>\n",
       "      <td>103.235</td>\n",
       "      <td>48.146</td>\n",
       "      <td>-999.00</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>3.473</td>\n",
       "      <td>2.078</td>\n",
       "      <td>...</td>\n",
       "      <td>0.725</td>\n",
       "      <td>1.158</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.00</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>46.226</td>\n",
       "      <td>0.681042</td>\n",
       "      <td>b</td>\n",
       "      <td>t</td>\n",
       "      <td>2.233584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100002</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>162.172</td>\n",
       "      <td>125.953</td>\n",
       "      <td>35.635</td>\n",
       "      <td>-999.00</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>3.148</td>\n",
       "      <td>9.336</td>\n",
       "      <td>...</td>\n",
       "      <td>2.053</td>\n",
       "      <td>-2.028</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.00</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>44.251</td>\n",
       "      <td>0.715742</td>\n",
       "      <td>b</td>\n",
       "      <td>t</td>\n",
       "      <td>2.347389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100003</td>\n",
       "      <td>143.905</td>\n",
       "      <td>81.417</td>\n",
       "      <td>80.943</td>\n",
       "      <td>0.414</td>\n",
       "      <td>-999.00</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>3.310</td>\n",
       "      <td>0.414</td>\n",
       "      <td>...</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.00</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>1.660654</td>\n",
       "      <td>b</td>\n",
       "      <td>t</td>\n",
       "      <td>5.446378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100004</td>\n",
       "      <td>175.864</td>\n",
       "      <td>16.915</td>\n",
       "      <td>134.805</td>\n",
       "      <td>16.405</td>\n",
       "      <td>-999.00</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>3.891</td>\n",
       "      <td>16.405</td>\n",
       "      <td>...</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.00</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.904263</td>\n",
       "      <td>b</td>\n",
       "      <td>t</td>\n",
       "      <td>6.245333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   EventId  DER_mass_MMC  DER_mass_transverse_met_lep  DER_mass_vis  DER_pt_h  \\\n",
       "0   100000       138.470                       51.655        97.827    27.980   \n",
       "1   100001       160.937                       68.768       103.235    48.146   \n",
       "2   100002      -999.000                      162.172       125.953    35.635   \n",
       "3   100003       143.905                       81.417        80.943     0.414   \n",
       "4   100004       175.864                       16.915       134.805    16.405   \n",
       "\n",
       "   DER_deltaeta_jet_jet  DER_mass_jet_jet  DER_prodeta_jet_jet  \\\n",
       "0                  0.91           124.711                2.666   \n",
       "1               -999.00          -999.000             -999.000   \n",
       "2               -999.00          -999.000             -999.000   \n",
       "3               -999.00          -999.000             -999.000   \n",
       "4               -999.00          -999.000             -999.000   \n",
       "\n",
       "   DER_deltar_tau_lep  DER_pt_tot      ...       PRI_jet_leading_eta  \\\n",
       "0               3.064      41.928      ...                     2.150   \n",
       "1               3.473       2.078      ...                     0.725   \n",
       "2               3.148       9.336      ...                     2.053   \n",
       "3               3.310       0.414      ...                  -999.000   \n",
       "4               3.891      16.405      ...                  -999.000   \n",
       "\n",
       "   PRI_jet_leading_phi  PRI_jet_subleading_pt  PRI_jet_subleading_eta  \\\n",
       "0                0.444                 46.062                    1.24   \n",
       "1                1.158               -999.000                 -999.00   \n",
       "2               -2.028               -999.000                 -999.00   \n",
       "3             -999.000               -999.000                 -999.00   \n",
       "4             -999.000               -999.000                 -999.00   \n",
       "\n",
       "   PRI_jet_subleading_phi  PRI_jet_all_pt    Weight  Label  KaggleSet  \\\n",
       "0                  -2.475         113.497  0.000814      s          t   \n",
       "1                -999.000          46.226  0.681042      b          t   \n",
       "2                -999.000          44.251  0.715742      b          t   \n",
       "3                -999.000          -0.000  1.660654      b          t   \n",
       "4                -999.000           0.000  1.904263      b          t   \n",
       "\n",
       "   KaggleWeight  \n",
       "0      0.002653  \n",
       "1      2.233584  \n",
       "2      2.347389  \n",
       "3      5.446378  \n",
       "4      6.245333  \n",
       "\n",
       "[5 rows x 35 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0xa1e8209e8>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAD8CAYAAABw1c+bAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAE8BJREFUeJzt3X+MHOV5wPHvU1MI5SiQkp6ojWojI1QLS01YQUir6q4N\nYFIMbYQquxaFFLCSCqk/kBqjVKL9o0rShiiJQkvdQGkq4gulNLEdR7RKcyWVEDVuU8yPOHGIE86k\nEIrq6igVcfP0j51zF2vXt3u7vt3Z9/uRTt55Z3bmfTx3z8098+47kZlIksrxQ8PugCRpeZn4Jakw\nJn5JKoyJX5IKY+KXpMKY+CWpMCZ+SSqMiV+SCmPil6TCnDLsDgCce+65uXr16rbrXn31Vc4444zl\n7dAyMK76GMeYwLjqpFNM+/btezkz39Lr/oaa+CNiI7Bx7dq1PPHEE223mZ2dZWpqaln7tRyMqz7G\nMSYwrjrpFFNEfHsp+xtqqSczd2Xm1rPOOmuY3ZCkoljjl6TCmPglqTAmfkkqjIlfkgpj4pekwgw1\n8UfExojYfuTIkWF2Q5KK4nBOSSrMSHxytx+rt32hbfuhD/3iMvdEkurBGr8kFcbEL0mFMfFLUmFM\n/JJUGBO/JBVm4Ik/IqYi4isRcU9ETA16/5Kk/nSV+CPivoh4KSKeOq59Q0QciIiDEbGtak5gHngT\nMDfY7kqS+tXtOP77gU8Cn15oiIgVwN3AFTQT/N6I2Al8JTP/MSImgY8CWwba4y45vl+S2ovM7G7D\niNXA7sy8uFq+HPj9zLyqWr4DIDM/WC2fCnwmM6/vsL+twFaAycnJS2ZmZtoed35+nomJiY792n+4\nt+ke1q8cjU8JLxZXXY1jXOMYExhXnXSKaXp6el9mNnrdXz+f3F0JPN+yPAdcFhHvBq4Czqb5V0Jb\nmbkd2A7QaDSy06PSFnuM2k0druw7ObSl876W0zg+Hg7GM65xjAmMq04GHVM/iT/atGVmPgw83NUO\nWp65K0laHv2M6pkDzm9ZXgW80MsOnKRNkpZfP4l/L3BhRKyp6vmbgJ297MBpmSVp+XU7nHMH8Bhw\nUUTMRcTNmXkUuA14BHgWeDAzn+7l4F7xS9Ly66rGn5mbO7TvAfYs9eDDqPF3GuYJDvWUVAYfxCJJ\nhXGuHkkqjM/claTCWOqRpMJY6pGkwljqkaTCWOqRpML0M1fP2HEqZ0klsNQjSYWx1CNJhXFUjyQV\nxsQvSYUx8UtSYby5K0mFGepwzszcBexqNBq3DrMfi3GYp6RxYqlHkgpj4pekwpj4JakwJn5JKoyJ\nX5IK43BOSSqMwzn74DBPSXVkqUeSCmPil6TCmPglqTAmfkkqjIlfkgpzUkb1RMQZwKPAnZm5+2Qc\nY5Q52kfSKOvqij8i7ouIlyLiqePaN0TEgYg4GBHbWla9H3hwkB2VJA1Gt6We+4ENrQ0RsQK4G7ga\nWAdsjoh1EfFO4BngxQH2U5I0IF2VejLz0YhYfVzzpcDBzHwOICJmgOuACeAMmr8MXouIPZn5g4H1\nWJLUl8jM7jZsJv7dmXlxtXw9sCEzb6mWbwAuy8zbquWbgJc71fgjYiuwFWBycvKSmZmZtsedn59n\nYmKiY7/2H67PdA/rV5517PVicdXVOMY1jjGBcdVJp5imp6f3ZWaj1/31c3M32rQd+y2Smfef6M2Z\nuR3YDtBoNHJqaqrtdrOzs3RaB3BThxupo+jQlqljrxeLq67GMa5xjAmMq04GHVM/iX8OOL9leRXw\nQi87iIiNwMa1a9f20Y36aB3tc/v6o8d+aTnaR9Jy6mcc/17gwohYExGnApuAnb3sIDN3ZebWs846\na/GNJUkD0e1wzh3AY8BFETEXETdn5lHgNuAR4Fngwcx8upeDOy2zJC2/bkf1bO7QvgfYs9SD131a\nZkmqIx/EIkmFGWrit8YvScvPSdokqTBDffRiacM5O3FSN0nLyVKPJBXGm7uSVBiv+CWpMN7claTC\nDPXmrk7Mm76STgZr/JJUGGv8klQYa/ySVBgTvyQVxsQvSYVxyoYacrSPpH54c1eSCmOpR5IKY+KX\npMKY+CWpMCZ+SSqMo3rGiKN9JHXDUT2SVBhLPZJUGBO/JBXG+fgLYO1fUiuv+CWpMCZ+SSqMiV+S\nCjPwxB8RPxUR90TEQxHxvkHvX5LUn64Sf0TcFxEvRcRTx7VviIgDEXEwIrYBZOazmfle4FeAxuC7\nLEnqR7ejeu4HPgl8eqEhIlYAdwNXAHPA3ojYmZnPRMS1wLbqPRpRjvaRytTVFX9mPgq8clzzpcDB\nzHwuM18HZoDrqu13ZuY7gC2D7KwkqX+Rmd1tGLEa2J2ZF1fL1wMbMvOWavkG4DLgIeDdwGnAk5l5\nd4f9bQW2AkxOTl4yMzPT9rjz8/NMTEx07Nf+w0e66v+omTwdXnxt2L1ob/3KpU+hsdj5qqNxjAmM\nq046xTQ9Pb0vM3suqffzAa5o05aZOQvMLvbmzNwObAdoNBo5NTXVdrvZ2Vk6rQO4qUO5YtTdvv4o\nd+0fzc/PHdoyteT3Lna+6mgcYwLjqpNBx9TPqJ454PyW5VXAC73sICI2RsT2I0fqedUuSXXUT+Lf\nC1wYEWsi4lRgE7BzMN2SJJ0sXdUaImIHMAWcGxFzwJ2ZeW9E3AY8AqwA7svMp3s5eGbuAnY1Go1b\ne+u2TqZOo33AET/SOOgq8Wfm5g7te4A9A+2RJOmkGuqUDdb4JWn5DXVYiaWe+vFDX1L9ecUvSYXx\nmbuSVBinZZakwgy1xh8RG4GNa9euHWY3NAALtf/b1x99w6eprf1Lo8dSjyQVxlKPJBXGxC9JhXE4\npyQVxhq/JBVmNCeE19jwk77S6LHGL0mFMfFLUmG8uStJhXF2Tg2FtX9peCz1SFJhTPySVBgTvyQV\nxnH8GinW/qWTz1E9klQYp2yQpMJY45ekwpj4JakwJn5JKoyJX5IK43BO1YLDPKXB8YpfkgpzUhJ/\nRPxSRPx5RHw+Iq48GceQJC1N16WeiLgPuAZ4KTMvbmnfAHwcWAF8KjM/lJmfAz4XEecAHwH+brDd\nlposAUm96+WK/35gQ2tDRKwA7gauBtYBmyNiXcsmv1etlySNiK4Tf2Y+CrxyXPOlwMHMfC4zXwdm\ngOui6cPAFzPzXwbXXUlSvyIzu984YjWwe6HUExHXAxsy85Zq+QbgMuDrwI3AXuCrmXlPm31tBbYC\nTE5OXjIzM9P2mPPz80xMTHTs0/7D9ZznZ/J0ePG1Yfdi8EYlrvUrBzcNyGLfg3VlXPXRKabp6el9\nmdnodX/9DueMNm2ZmZ8APnGiN2bm9oj4LrDxzDPPvGRqaqrtdrOzs3RaB3BThxrvqLt9/VHu2j9+\no2lHJa5DW6YGtq/FvgfryrjqY9Ax9fsTOgec37K8Cnih2zf76EWdLN70lTrrN/HvBS6MiDXAYWAT\n8KvdvjkiNgIb165d22c3pO74C0Hq4eZuROwAHgMuioi5iLg5M48CtwGPAM8CD2bm093u02mZJWn5\ndX3Fn5mbO7TvAfYs5eBe8UvS8vNBLJJUGOfqkaTC+MxdSSqMpR5JKoylHkkqzFA/YumoHo0Kx/er\nJJZ6JKkww59URRphq7d9gdvXH207J5R/DaiurPFLUmEczilJhbHGL0mFsdQjSYXx5q40YA4N1aiz\nxi9JhbHGL0mFscYvSYUx8UtSYUz8klQYE78kFcbEL0mFcTinJBXG4ZySVBhLPZJUGKdskJao09QM\n0qjzil+SCmPil6TCmPglqTADT/wRcUFE3BsRDw1635Kk/nV1czci7gOuAV7KzItb2jcAHwdWAJ/K\nzA9l5nPAzSZ+6Y2cp1+jotsr/vuBDa0NEbECuBu4GlgHbI6IdQPtnSRp4Lq64s/MRyNi9XHNlwIH\nqyt8ImIGuA54ZpAdlMadfwlouUVmdrdhM/HvXij1RMT1wIbMvKVavgG4DLgT+EPgCprlnw922N9W\nYCvA5OTkJTMzM22POz8/z8TERMd+7T9cz+keJk+HF18bdi8GbxzjGlZM61ee3E+0L/azVVfjGFen\nmKanp/dlZqPX/fXzAa5o05aZ+R/Aexd7c2ZuB7YDNBqNnJqaarvd7OwsndYB3FTTD9Hcvv4od+0f\nv8/PjWNcw4rp0Japk7r/xX626moc4xp0TP2M6pkDzm9ZXgW80MsOnKRNkpZfP4l/L3BhRKyJiFOB\nTcDOXnbgJG2StPy6Hc65A5gCzo2IOeDOzLw3Im4DHqE5nPO+zHy6l4NHxEZg49q1a3vrtVQwbwar\nX92O6tncoX0PsGepB8/MXcCuRqNx61L3IUnqjQ9ikaTC+CAWSSqMk7RJUmEs9UhSYSz1SFJhLPVI\nUmEs9UhSYSz1SFJhLPVIUmGGOo2iUzZInXWamkHql6UeSSqMpR5JKoyJX5IKY+KXpMJ4c1cq1P7D\nR9o+utR5/cefN3clqTCWeiSpMCZ+SSqMiV+SCmPil6TCmPglqTAO55TGRKe5fXodnjnIOYIGdWyH\nmA6WwzklqTCWeiSpMCZ+SSqMiV+SCmPil6TCmPglqTADH84ZEWcAfwK8Dsxm5gODPoYkaem6uuKP\niPsi4qWIeOq49g0RcSAiDkbEtqr53cBDmXkrcO2A+ytJ6lO3pZ77gQ2tDRGxArgbuBpYB2yOiHXA\nKuD5arP/HUw3JUmD0lXiz8xHgVeOa74UOJiZz2Xm68AMcB0wRzP5d71/SdLyiczsbsOI1cDuzLy4\nWr4e2JCZt1TLNwCXAe8HPgn8D/BPnWr8EbEV2AowOTl5yczMTNvjzs/PMzEx0bFf+w8f6ar/o2by\ndHjxtWH3YvDGMa5xjAmWJ671K9t/Kr/Xn9tO+2nnRDmj03F72f+JLCUfdXPsTjFNT0/vy8xGr8fs\n5+ZutGnLzHwVeM9ib87M7cB2gEajkVNTU223m52dpdM6oO2j4+rg9vVHuWv/UKdKOinGMa5xjAmW\nJ65DW6batvf6c9tpP+2cKGd0Om4v+z+RpeSjbo69WB7sVT+lmDng/JblVcALvewgIjZGxPYjR+p5\n1S5JddRP4t8LXBgRayLiVGATsLOXHThJmyQtv26Hc+4AHgMuioi5iLg5M48CtwGPAM8CD2bm070c\n3Ct+SVp+XRX4MnNzh/Y9wJ6lHjwzdwG7Go3GrUvdhySpN0MdbukVvyQtPx/EIkmF8YpfkgrjFb8k\nFabrT+6e1E5EfA/4dofV5wIvL2N3lotx1cc4xgTGVSedYvrJzHxLrzsbicR/IhHxxFI+kjzqjKs+\nxjEmMK46GXRMTqImSYUx8UtSYeqQ+LcPuwMniXHVxzjGBMZVJwONaeRr/JKkwarDFb8kaYBGOvF3\neKbvyIuI8yPiyxHxbEQ8HRG/WbW/OSL+PiK+Uf17TtUeEfGJKs4nI+Jtw43gxCJiRUT8a0TsrpbX\nRMTjVVyfrWZrJSJOq5YPVutXD7PfJxIRZ0fEQxHxteq8XV738xURv119/z0VETsi4k11PFftnvm9\nlHMTETdW238jIm4cRiytOsT1x9X34JMR8bcRcXbLujuquA5ExFUt7b3nycwcyS9gBfBN4ALgVODf\ngHXD7leXfT8PeFv1+kzg6zSfS/xHwLaqfRvw4er1u4Av0ny4zduBx4cdwyLx/Q7wGZpPZAN4ENhU\nvb4HeF/1+jeAe6rXm4DPDrvvJ4jpL4FbqtenAmfX+XwBK4FvAae3nKOb6niugJ8D3gY81dLW07kB\n3gw8V/17TvX6nBGM60rglOr1h1viWlflwNOANVVuXLHUPDn0k3qC/5TLgUdalu8A7hh2v5YYy+eB\nK4ADwHlV23nAger1nwGbW7Y/tt2ofdF84M6XgJ8Hdlc/YC+3fLMeO280p+y+vHp9SrVdDDuGNjH9\naJUk47j22p6vKvE/XyW6U6pzdVVdzxWw+rgE2dO5ATYDf9bS/obtRiWu49b9MvBA9foN+W/hfC01\nT45yqWfhG3fBXNVWK9WfzG8FHgcmM/O7ANW/P15tVqdYPwb8LvCDavnHgP/M5vMZ4I19PxZXtf5I\ntf2ouQD4HvAXVQnrUxFxBjU+X5l5GPgI8B3guzT/7/dR/3O1oNdzM/LnrI1fp/nXCww4rlFO/G2f\n6bvsvehDREwAfwP8Vmb+14k2bdM2crFGxDXAS5m5r7W5zabZxbpRcgrNP7n/NDPfCrxKs3zQycjH\nVdW8r6NZFvgJ4Azg6jab1u1cLaZTHLWKLyI+ABwFHlhoarPZkuMa5cTf9zN9hykifphm0n8gMx+u\nml+MiPOq9ecBL1XtdYn1Z4BrI+IQMEOz3PMx4OyIWHioT2vfj8VVrT8LeGU5O9ylOWAuMx+vlh+i\n+YugzufrncC3MvN7mfl94GHgHdT/XC3o9dzU4ZwBzZvQwDXAlqzqNww4rlFO/H0/03dYIiKAe4Fn\nM/OjLat2AgujCW6kWftfaP+1akTC24EjC3/GjpLMvCMzV2Xmaprn4x8ycwvwZeD6arPj41qI9/pq\n+5G7ysrMfweej4iLqqZfAJ6h3ufrO8DbI+JHqu/HhZhqfa5a9HpuHgGujIhzqr+GrqzaRkpEbADe\nD1ybmf/dsmonsKkafbUGuBD4Z5aaJ4d9c2ORGx/vojki5pvAB4bdnx76/bM0/9x6Evhq9fUumjXT\nLwHfqP59c7V9AHdXce4HGsOOoYsYp/j/UT0XVN+EB4G/Bk6r2t9ULR+s1l8w7H6fIJ6fBp6oztnn\naI78qPX5Av4A+BrwFPBXNEeE1O5cATto3qf4Ps0r3JuXcm5o1swPVl/vGdG4DtKs2S/kjXtatv9A\nFdcB4OqW9p7zpJ/claTCjHKpR5J0Epj4JakwJn5JKoyJX5IKY+KXpMKY+CWpMCZ+SSqMiV+SCvN/\nSV7l/WdbTtwAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xa1e70b048>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAD8CAYAAACcjGjIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xa1fbfbe80>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.PRI_jet_leading_pt[df.PRI_jet_leading_pt>0].hist(bins=50)\n",
    "plt.yscale('log')\n",
    "\n",
    "f=plt.figure()\n",
    "df.DER_mass_MMC[(df.DER_mass_MMC>0)&(df.DER_mass_MMC<250)].hist(bins=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is more information about the variables in the documentation. The variables that start with **DER** are derived quantities, determined by the physicists performing the analysis as variables that discriminate signal from backround. On the other hand, those that start with **PRI** are considered to be primary variables, from which the derived variables are calculated. They themselves generally do not provide much discrimination, but one if the ideas suggested by deep networks is that they can determine the necessary features from the primary variables, potentially even finding variables that the physicists did not consider. *EventId* identifies the event but is not a \"feature.\" The *Weight* is the event weight so that the sum of weights of all signal events should produce the signal yield expected to be observed in 2012, and the sum of weights of all background events should produce the backgroudn yield. Note that the weight varies event to event, because different background and signal processes contribute to the background and signal sets. *Label* indicates if it is a signal or background event. Ignore the *Kaggle* variables--they are only used if you want to reproduce exactly what was used in the Challenge. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map y values to integers\n",
    "df['Label'] = df['Label'].map({'b':0, 's':1})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EventId</th>\n",
       "      <th>DER_mass_MMC</th>\n",
       "      <th>DER_mass_transverse_met_lep</th>\n",
       "      <th>DER_mass_vis</th>\n",
       "      <th>DER_pt_h</th>\n",
       "      <th>DER_deltaeta_jet_jet</th>\n",
       "      <th>DER_mass_jet_jet</th>\n",
       "      <th>DER_prodeta_jet_jet</th>\n",
       "      <th>DER_deltar_tau_lep</th>\n",
       "      <th>DER_pt_tot</th>\n",
       "      <th>...</th>\n",
       "      <th>PRI_jet_leading_eta</th>\n",
       "      <th>PRI_jet_leading_phi</th>\n",
       "      <th>PRI_jet_subleading_pt</th>\n",
       "      <th>PRI_jet_subleading_eta</th>\n",
       "      <th>PRI_jet_subleading_phi</th>\n",
       "      <th>PRI_jet_all_pt</th>\n",
       "      <th>Weight</th>\n",
       "      <th>Label</th>\n",
       "      <th>KaggleSet</th>\n",
       "      <th>KaggleWeight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100000</td>\n",
       "      <td>138.470</td>\n",
       "      <td>51.655</td>\n",
       "      <td>97.827</td>\n",
       "      <td>27.980</td>\n",
       "      <td>0.91</td>\n",
       "      <td>124.711</td>\n",
       "      <td>2.666</td>\n",
       "      <td>3.064</td>\n",
       "      <td>41.928</td>\n",
       "      <td>...</td>\n",
       "      <td>2.150</td>\n",
       "      <td>0.444</td>\n",
       "      <td>46.062</td>\n",
       "      <td>1.24</td>\n",
       "      <td>-2.475</td>\n",
       "      <td>113.497</td>\n",
       "      <td>0.000814</td>\n",
       "      <td>1</td>\n",
       "      <td>t</td>\n",
       "      <td>0.002653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100001</td>\n",
       "      <td>160.937</td>\n",
       "      <td>68.768</td>\n",
       "      <td>103.235</td>\n",
       "      <td>48.146</td>\n",
       "      <td>-999.00</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>3.473</td>\n",
       "      <td>2.078</td>\n",
       "      <td>...</td>\n",
       "      <td>0.725</td>\n",
       "      <td>1.158</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.00</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>46.226</td>\n",
       "      <td>0.681042</td>\n",
       "      <td>0</td>\n",
       "      <td>t</td>\n",
       "      <td>2.233584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100002</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>162.172</td>\n",
       "      <td>125.953</td>\n",
       "      <td>35.635</td>\n",
       "      <td>-999.00</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>3.148</td>\n",
       "      <td>9.336</td>\n",
       "      <td>...</td>\n",
       "      <td>2.053</td>\n",
       "      <td>-2.028</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.00</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>44.251</td>\n",
       "      <td>0.715742</td>\n",
       "      <td>0</td>\n",
       "      <td>t</td>\n",
       "      <td>2.347389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100003</td>\n",
       "      <td>143.905</td>\n",
       "      <td>81.417</td>\n",
       "      <td>80.943</td>\n",
       "      <td>0.414</td>\n",
       "      <td>-999.00</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>3.310</td>\n",
       "      <td>0.414</td>\n",
       "      <td>...</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.00</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>1.660654</td>\n",
       "      <td>0</td>\n",
       "      <td>t</td>\n",
       "      <td>5.446378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100004</td>\n",
       "      <td>175.864</td>\n",
       "      <td>16.915</td>\n",
       "      <td>134.805</td>\n",
       "      <td>16.405</td>\n",
       "      <td>-999.00</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>3.891</td>\n",
       "      <td>16.405</td>\n",
       "      <td>...</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.00</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.904263</td>\n",
       "      <td>0</td>\n",
       "      <td>t</td>\n",
       "      <td>6.245333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   EventId  DER_mass_MMC  DER_mass_transverse_met_lep  DER_mass_vis  DER_pt_h  \\\n",
       "0   100000       138.470                       51.655        97.827    27.980   \n",
       "1   100001       160.937                       68.768       103.235    48.146   \n",
       "2   100002      -999.000                      162.172       125.953    35.635   \n",
       "3   100003       143.905                       81.417        80.943     0.414   \n",
       "4   100004       175.864                       16.915       134.805    16.405   \n",
       "\n",
       "   DER_deltaeta_jet_jet  DER_mass_jet_jet  DER_prodeta_jet_jet  \\\n",
       "0                  0.91           124.711                2.666   \n",
       "1               -999.00          -999.000             -999.000   \n",
       "2               -999.00          -999.000             -999.000   \n",
       "3               -999.00          -999.000             -999.000   \n",
       "4               -999.00          -999.000             -999.000   \n",
       "\n",
       "   DER_deltar_tau_lep  DER_pt_tot      ...       PRI_jet_leading_eta  \\\n",
       "0               3.064      41.928      ...                     2.150   \n",
       "1               3.473       2.078      ...                     0.725   \n",
       "2               3.148       9.336      ...                     2.053   \n",
       "3               3.310       0.414      ...                  -999.000   \n",
       "4               3.891      16.405      ...                  -999.000   \n",
       "\n",
       "   PRI_jet_leading_phi  PRI_jet_subleading_pt  PRI_jet_subleading_eta  \\\n",
       "0                0.444                 46.062                    1.24   \n",
       "1                1.158               -999.000                 -999.00   \n",
       "2               -2.028               -999.000                 -999.00   \n",
       "3             -999.000               -999.000                 -999.00   \n",
       "4             -999.000               -999.000                 -999.00   \n",
       "\n",
       "   PRI_jet_subleading_phi  PRI_jet_all_pt    Weight  Label  KaggleSet  \\\n",
       "0                  -2.475         113.497  0.000814      1          t   \n",
       "1                -999.000          46.226  0.681042      0          t   \n",
       "2                -999.000          44.251  0.715742      0          t   \n",
       "3                -999.000          -0.000  1.660654      0          t   \n",
       "4                -999.000           0.000  1.904263      0          t   \n",
       "\n",
       "   KaggleWeight  \n",
       "0      0.002653  \n",
       "1      2.233584  \n",
       "2      2.347389  \n",
       "3      5.446378  \n",
       "4      6.245333  \n",
       "\n",
       "[5 rows x 35 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's create separate arrays\n",
    "eventID = df['EventId']\n",
    "X = df.loc[:,'DER_mass_MMC':'PRI_jet_all_pt']\n",
    "y = df['Label']\n",
    "weight = df['Weight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now split into testing and training samples\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test, eventID_train, event_ID_test, weight_train, weight_test = train_test_split(\n",
    "    X, y, eventID, weight, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let's first look at a NN in sklearn\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "mlp = MLPClassifier(verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.99806865\n",
      "Iteration 2, loss = 0.75124710\n",
      "Iteration 3, loss = 0.69704370\n",
      "Iteration 4, loss = 0.64347047\n",
      "Iteration 5, loss = 0.61136224\n",
      "Iteration 6, loss = 0.60206532\n",
      "Iteration 7, loss = 0.56544541\n",
      "Iteration 8, loss = 0.54158950\n",
      "Iteration 9, loss = 0.52514796\n",
      "Iteration 10, loss = 0.51546395\n",
      "Iteration 11, loss = 0.50221566\n",
      "Iteration 12, loss = 0.48586294\n",
      "Iteration 13, loss = 0.48009284\n",
      "Iteration 14, loss = 0.47298407\n",
      "Iteration 15, loss = 0.46790751\n",
      "Iteration 16, loss = 0.45870702\n",
      "Iteration 17, loss = 0.44935794\n",
      "Iteration 18, loss = 0.44631367\n",
      "Iteration 19, loss = 0.44113827\n",
      "Iteration 20, loss = 0.43488199\n",
      "Iteration 21, loss = 0.43356850\n",
      "Iteration 22, loss = 0.42680669\n",
      "Iteration 23, loss = 0.42590778\n",
      "Iteration 24, loss = 0.42029429\n",
      "Iteration 25, loss = 0.41776486\n",
      "Iteration 26, loss = 0.41358780\n",
      "Iteration 27, loss = 0.41326829\n",
      "Iteration 28, loss = 0.41062729\n",
      "Iteration 29, loss = 0.40941320\n",
      "Iteration 30, loss = 0.40811012\n",
      "Iteration 31, loss = 0.40751565\n",
      "Iteration 32, loss = 0.40821387\n",
      "Iteration 33, loss = 0.40661443\n",
      "Iteration 34, loss = 0.40544161\n",
      "Iteration 35, loss = 0.40529796\n",
      "Iteration 36, loss = 0.40270902\n",
      "Iteration 37, loss = 0.40193983\n",
      "Iteration 38, loss = 0.40130948\n",
      "Iteration 39, loss = 0.40058119\n",
      "Iteration 40, loss = 0.39907968\n",
      "Iteration 41, loss = 0.39942935\n",
      "Iteration 42, loss = 0.39909282\n",
      "Iteration 43, loss = 0.39826025\n",
      "Iteration 44, loss = 0.39658269\n",
      "Iteration 45, loss = 0.39734180\n",
      "Iteration 46, loss = 0.39599099\n",
      "Iteration 47, loss = 0.39553619\n",
      "Iteration 48, loss = 0.39582442\n",
      "Iteration 49, loss = 0.39532500\n",
      "Iteration 50, loss = 0.39519181\n",
      "Iteration 51, loss = 0.39498501\n",
      "Iteration 52, loss = 0.39483777\n",
      "Iteration 53, loss = 0.39517029\n",
      "Iteration 54, loss = 0.39372363\n",
      "Iteration 55, loss = 0.39398054\n",
      "Iteration 56, loss = 0.39385130\n",
      "Iteration 57, loss = 0.39305275\n",
      "Iteration 58, loss = 0.39320376\n",
      "Iteration 59, loss = 0.39255569\n",
      "Iteration 60, loss = 0.39240046\n",
      "Iteration 61, loss = 0.39196283\n",
      "Iteration 62, loss = 0.39262291\n",
      "Iteration 63, loss = 0.39191581\n",
      "Iteration 64, loss = 0.39329429\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
       "       nesterovs_momentum=True, power_t=0.5, random_state=None,\n",
       "       shuffle=True, solver='adam', tol=0.0001, validation_fraction=0.1,\n",
       "       verbose=True, warm_start=False)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# and train\n",
    "mlp.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.82175328402816095"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Kaggle competition used the approximate median segnificance (AMS), as defined below, to determine how good a solution was. The number 10, added to the background yield, is a regularization term to decrease the variance of the AMS.\n",
    "\n",
    "Note that if you do not use the full data set (i.e. you split into training and testing) you have to reweigh the inputs so that the subsample yield matches to the toal yield, which we will do below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute approximate median segnificance (AMS)\n",
    "\n",
    "def ams(s,b):\n",
    "    from math import sqrt,log\n",
    "    if b==0:\n",
    "        return 0\n",
    "\n",
    "    return sqrt(2*((s+b+10)*log(1+float(s)/(b+10))-s))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.79104873568957912"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's try a different probability cut, not the one given by default to predict().\n",
    "# We choose the top 15%, but can optimize\n",
    "y_train_prob = mlp.predict_proba(X_train)[:, 1]\n",
    "y_test_prob = mlp.predict_proba(X_test)[:, 1]\n",
    "pcut = np.percentile(y_train_prob,85)\n",
    "pcut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the probability to the original data frame\n",
    "df['Prob']=mlp.predict_proba(X)[:, 1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0xa1efde908>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW4AAAD8CAYAAABXe05zAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xa5fbe75c0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "kwargs = dict(histtype='stepfilled', alpha=0.3, normed=True, bins=40)\n",
    "\n",
    "df[df.Label==0].Prob.hist(label='Background',**kwargs)\n",
    "df[df.Label==1].Prob.hist(label='Signal',**kwargs)\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's calculate the total weights (yields)\n",
    "sigall = weight.dot(y)\n",
    "backall = weight.dot(y == 0)\n",
    "\n",
    "# The training weights\n",
    "sigtrain = weight_train.dot(y_train)\n",
    "backtrain = weight_train.dot(y_train == 0)\n",
    "\n",
    "# The training weights\n",
    "sigtest = weight_test.dot(y_test)\n",
    "backtest = weight_test.dot(y_test == 0)\n",
    "\n",
    "# aside:  these can also be done by looping instead of using a dot product\n",
    "#  (Usually vectorized operations are faster for interpreted code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's look at event yields that pass our selection\n",
    "sigtrain_sel = weight_train.dot(np.multiply(y_train, y_train_prob > pcut))\n",
    "backtrain_sel = weight_train.dot(np.multiply(y_train == 0, y_train_prob > pcut))\n",
    "\n",
    "sigtest_sel = weight_test.dot(np.multiply(y_test, y_test_prob > pcut))\n",
    "backtest_sel = weight_test.dot(np.multiply(y_test == 0, y_test_prob > pcut))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corrected selected yields in training sample, signal = 193.401584758 , background = 4469.12022477\n",
      "Corrected selected yields in test sample, signal = 194.337526339 , background = 4634.32066731\n"
     ]
    }
   ],
   "source": [
    "# Now we need to correct the selected yields to be is if we used the full sample\n",
    "sigtrain_sel_corr = sigtrain_sel*sigall/sigtrain\n",
    "backtrain_sel_corr = backtrain_sel*backall/backtrain\n",
    "\n",
    "sigtest_sel_corr = sigtest_sel*sigall/sigtest\n",
    "backtest_sel_corr = backtest_sel*backall/backtest\n",
    "\n",
    "print(\"Corrected selected yields in training sample, signal =\", sigtrain_sel_corr, \", background =\",backtrain_sel_corr)\n",
    "print(\"Corrected selected yields in test sample, signal =\", sigtest_sel_corr, \", background =\",backtest_sel_corr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AMS of training sample 2.86934204371926\n",
      "AMS of test sample 2.832097218084367\n"
     ]
    }
   ],
   "source": [
    "print(\"AMS of training sample\", ams(sigtrain_sel_corr,backtrain_sel_corr))\n",
    "print(\"AMS of test sample\", ams(sigtest_sel_corr,backtest_sel_corr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How did we do? Worse than the BDT from yesterday.\n",
    "![Comparison with submissions](data/tr150908_davidRousseau_TMVAFuture_HiggsML.001.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural networks are quite sensitive to feature scaling, so let's try to scale the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import RobustScaler\n",
    "scaler = RobustScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.40670887\n",
      "Iteration 2, loss = 0.37873972\n",
      "Iteration 3, loss = 0.37302798\n",
      "Iteration 4, loss = 0.36929403\n",
      "Iteration 5, loss = 0.36738427\n",
      "Iteration 6, loss = 0.36595492\n",
      "Iteration 7, loss = 0.36497659\n",
      "Iteration 8, loss = 0.36428119\n",
      "Iteration 9, loss = 0.36360813\n",
      "Iteration 10, loss = 0.36335834\n",
      "Iteration 11, loss = 0.36283169\n",
      "Iteration 12, loss = 0.36239322\n",
      "Iteration 13, loss = 0.36201070\n",
      "Iteration 14, loss = 0.36168348\n",
      "Iteration 15, loss = 0.36140978\n",
      "Iteration 16, loss = 0.36122838\n",
      "Iteration 17, loss = 0.36097433\n",
      "Iteration 18, loss = 0.36076444\n",
      "Iteration 19, loss = 0.36063856\n",
      "Iteration 20, loss = 0.36038491\n",
      "Iteration 21, loss = 0.36023841\n",
      "Iteration 22, loss = 0.35996516\n",
      "Iteration 23, loss = 0.36004249\n",
      "Iteration 24, loss = 0.35970522\n",
      "Iteration 25, loss = 0.35954031\n",
      "Iteration 26, loss = 0.35945109\n",
      "Iteration 27, loss = 0.35942376\n",
      "Iteration 28, loss = 0.35923255\n",
      "Iteration 29, loss = 0.35915821\n",
      "Iteration 30, loss = 0.35899247\n",
      "Iteration 31, loss = 0.35885957\n",
      "Iteration 32, loss = 0.35868797\n",
      "Iteration 33, loss = 0.35865783\n",
      "Iteration 34, loss = 0.35859532\n",
      "Iteration 35, loss = 0.35844392\n",
      "Iteration 36, loss = 0.35833842\n",
      "Iteration 37, loss = 0.35830415\n",
      "Iteration 38, loss = 0.35820962\n",
      "Iteration 39, loss = 0.35810311\n",
      "Iteration 40, loss = 0.35797509\n",
      "Iteration 41, loss = 0.35797499\n",
      "Iteration 42, loss = 0.35785437\n",
      "Iteration 43, loss = 0.35773725\n",
      "Iteration 44, loss = 0.35776506\n",
      "Iteration 45, loss = 0.35763622\n",
      "Iteration 46, loss = 0.35768457\n",
      "Iteration 47, loss = 0.35755125\n",
      "Iteration 48, loss = 0.35747866\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
       "       nesterovs_momentum=True, power_t=0.5, random_state=None,\n",
       "       shuffle=True, solver='adam', tol=0.0001, validation_fraction=0.1,\n",
       "       verbose=True, warm_start=False)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# and train a new network\n",
    "mlp_scaled = MLPClassifier(verbose=True)\n",
    "mlp_scaled.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.83808917150274609"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp_scaled.score(X_test_scaled, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's try a different probability cut, not the one given by default to predict().\n",
    "# We choose the top 15%, but can optimize\n",
    "y_train_prob_scaled = mlp_scaled.predict_proba(X_train_scaled)[:, 1]\n",
    "y_test_prob_scaled = mlp_scaled.predict_proba(X_test_scaled)[:, 1]\n",
    "pcut_scaled = np.percentile(y_train_prob_scaled,85)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's look at event yields that pass our selection\n",
    "sigtrain_sel_scaled = weight_train.dot(np.multiply(y_train, y_train_prob_scaled > pcut_scaled))\n",
    "backtrain_sel_scaled = weight_train.dot(np.multiply(y_train == 0, y_train_prob_scaled > pcut_scaled))\n",
    "\n",
    "sigtest_sel_scaled = weight_test.dot(np.multiply(y_test, y_test_prob_scaled > pcut_scaled))\n",
    "backtest_sel_scaled = weight_test.dot(np.multiply(y_test == 0, y_test_prob_scaled > pcut_scaled))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corrected selected yields in training sample, signal = 230.996354566 , background = 4196.64408154\n",
      "Corrected selected yields in test sample, signal = 230.174355902 , background = 4442.53485662\n"
     ]
    }
   ],
   "source": [
    "# Now we need to correct the selected yields to be is if we used the full sample\n",
    "sigtrain_sel_scaled_corr = sigtrain_sel_scaled*sigall/sigtrain\n",
    "backtrain_sel_scaled_corr = backtrain_sel_scaled*backall/backtrain\n",
    "\n",
    "sigtest_sel_scaled_corr = sigtest_sel_scaled*sigall/sigtest\n",
    "backtest_sel_scaled_corr = backtest_sel_scaled*backall/backtest\n",
    "\n",
    "print(\"Corrected selected yields in training sample, signal =\", sigtrain_sel_scaled_corr, \", background =\",backtrain_sel_scaled_corr)\n",
    "print(\"Corrected selected yields in test sample, signal =\", sigtest_sel_scaled_corr, \", background =\",backtest_sel_scaled_corr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AMS of training sample 3.529663769204157\n",
      "AMS of test sample 3.4203793909442077\n"
     ]
    }
   ],
   "source": [
    "print(\"AMS of training sample\", ams(sigtrain_sel_scaled_corr,backtrain_sel_scaled_corr))\n",
    "print(\"AMS of test sample\", ams(sigtest_sel_scaled_corr,backtest_sel_scaled_corr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We improved somewhat.\n",
    "\n",
    "SciKit Learn has simple NNs, but if you want to do deep NNs, or train on GPUs, you probalby want to use something like Keras instead. Let's try to create a simple NN using Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1337)  # for reproducibility\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the model\n",
    "model = Sequential()\n",
    "model.add(Dense(100, activation='relu', input_shape=(30,)))\n",
    "model.add(Dense(1, activation='sigmoid'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "548219/548219 [==============================] - 10s 18us/step - loss: 0.3984 - acc: 0.8212\n",
      "Epoch 2/20\n",
      "548219/548219 [==============================] - 9s 17us/step - loss: 0.3778 - acc: 0.8315\n",
      "Epoch 3/20\n",
      "548219/548219 [==============================] - 9s 17us/step - loss: 0.3732 - acc: 0.8333\n",
      "Epoch 4/20\n",
      "548219/548219 [==============================] - 10s 18us/step - loss: 0.3705 - acc: 0.8349\n",
      "Epoch 5/20\n",
      "548219/548219 [==============================] - 13s 23us/step - loss: 0.3678 - acc: 0.8357\n",
      "Epoch 6/20\n",
      "548219/548219 [==============================] - 11s 20us/step - loss: 0.3660 - acc: 0.8363\n",
      "Epoch 7/20\n",
      "548219/548219 [==============================] - 11s 19us/step - loss: 0.3650 - acc: 0.8366\n",
      "Epoch 8/20\n",
      "548219/548219 [==============================] - 10s 19us/step - loss: 0.3644 - acc: 0.8369\n",
      "Epoch 9/20\n",
      "548219/548219 [==============================] - 10s 18us/step - loss: 0.3637 - acc: 0.8370\n",
      "Epoch 10/20\n",
      "548219/548219 [==============================] - 10s 18us/step - loss: 0.3632 - acc: 0.8376\n",
      "Epoch 11/20\n",
      "548219/548219 [==============================] - 10s 18us/step - loss: 0.3628 - acc: 0.8378\n",
      "Epoch 12/20\n",
      "548219/548219 [==============================] - 10s 18us/step - loss: 0.3624 - acc: 0.8378\n",
      "Epoch 13/20\n",
      "548219/548219 [==============================] - 10s 18us/step - loss: 0.3620 - acc: 0.8379\n",
      "Epoch 14/20\n",
      "548219/548219 [==============================] - 11s 19us/step - loss: 0.3616 - acc: 0.8383\n",
      "Epoch 15/20\n",
      "548219/548219 [==============================] - 10s 18us/step - loss: 0.3614 - acc: 0.8385\n",
      "Epoch 16/20\n",
      "548219/548219 [==============================] - 10s 18us/step - loss: 0.3610 - acc: 0.8386\n",
      "Epoch 17/20\n",
      "548219/548219 [==============================] - 10s 18us/step - loss: 0.3607 - acc: 0.8385\n",
      "Epoch 18/20\n",
      "548219/548219 [==============================] - 10s 19us/step - loss: 0.3605 - acc: 0.8388\n",
      "Epoch 19/20\n",
      "548219/548219 [==============================] - 10s 18us/step - loss: 0.3603 - acc: 0.8388\n",
      "Epoch 20/20\n",
      "548219/548219 [==============================] - 10s 19us/step - loss: 0.3602 - acc: 0.8389\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0xa5fbe7c50>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train_scaled, y_train, epochs=20, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's try a different probability cut, not the one given by default to predict().\n",
    "# We choose the top 15%, but can optimize\n",
    "y_train_prob_keras = model.predict(X_train_scaled)[:, 0]\n",
    "y_test_prob_keras = model.predict(X_test_scaled)[:, 0]\n",
    "pcut_keras = np.percentile(y_train_prob_keras,85)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's look at event yields that pass our selection\n",
    "sigtrain_sel_keras = weight_train.dot(np.multiply(y_train, y_train_prob_keras > pcut_keras))\n",
    "backtrain_sel_keras = weight_train.dot(np.multiply(y_train == 0, y_train_prob_keras > pcut_keras))\n",
    "\n",
    "sigtest_sel_keras = weight_test.dot(np.multiply(y_test, y_test_prob_keras > pcut_keras))\n",
    "backtest_sel_keras = weight_test.dot(np.multiply(y_test == 0, y_test_prob_keras > pcut_keras))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corrected selected yields in training sample, signal = 238.653776 , background = 4693.59609754\n",
      "Corrected selected yields in test sample, signal = 237.905045508 , background = 4971.94230648\n"
     ]
    }
   ],
   "source": [
    "# Now we need to correct the selected yields to be is if we used the full sample\n",
    "sigtrain_sel_keras_corr = sigtrain_sel_keras*sigall/sigtrain\n",
    "backtrain_sel_keras_corr = backtrain_sel_keras*backall/backtrain\n",
    "\n",
    "sigtest_sel_keras_corr = sigtest_sel_keras*sigall/sigtest\n",
    "backtest_sel_keras_corr = backtest_sel_keras*backall/backtest\n",
    "\n",
    "print(\"Corrected selected yields in training sample, signal =\", sigtrain_sel_keras_corr, \", background =\",backtrain_sel_keras_corr)\n",
    "print(\"Corrected selected yields in test sample, signal =\", sigtest_sel_keras_corr, \", background =\",backtest_sel_keras_corr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AMS of training sample 3.4509706870281347\n",
      "AMS of test sample 3.34427135944795\n"
     ]
    }
   ],
   "source": [
    "print(\"AMS of training sample\", ams(sigtrain_sel_keras_corr,backtrain_sel_keras_corr))\n",
    "print(\"AMS of test sample\", ams(sigtest_sel_keras_corr,backtest_sel_keras_corr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We only made a single layer NN in Keras. However, you can easily change the structure of the network. As an assignment, try adding an extra hidden layer and changing the number of neurons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a few things we can easily vary: number of hidden layers, the activation function, the regularization ($\\alpha$). Let's go back to MLPClassifer (scaled) and play with some of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.38988645\n",
      "Iteration 2, loss = 0.37209813\n",
      "Iteration 3, loss = 0.36866211\n",
      "Iteration 4, loss = 0.36662743\n",
      "Iteration 5, loss = 0.36519167\n",
      "Iteration 6, loss = 0.36420247\n",
      "Iteration 7, loss = 0.36365862\n",
      "Iteration 8, loss = 0.36301438\n",
      "Iteration 9, loss = 0.36250376\n",
      "Iteration 10, loss = 0.36234049\n",
      "Iteration 11, loss = 0.36197566\n",
      "Iteration 12, loss = 0.36163091\n",
      "Iteration 13, loss = 0.36146661\n",
      "Iteration 14, loss = 0.36108755\n",
      "Iteration 15, loss = 0.36105851\n",
      "Iteration 16, loss = 0.36086552\n",
      "Iteration 17, loss = 0.36060469\n",
      "Iteration 18, loss = 0.36051176\n",
      "Iteration 19, loss = 0.36040121\n",
      "Iteration 20, loss = 0.36036926\n",
      "Iteration 21, loss = 0.36006069\n",
      "Iteration 22, loss = 0.36024082\n",
      "Iteration 23, loss = 0.36014752\n",
      "Iteration 24, loss = 0.35992278\n",
      "Iteration 25, loss = 0.35967233\n",
      "Iteration 26, loss = 0.35987269\n",
      "Iteration 27, loss = 0.35960379\n",
      "Iteration 28, loss = 0.35965437\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=0.01, batch_size='auto', beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(100, 100), learning_rate='constant',\n",
       "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
       "       nesterovs_momentum=True, power_t=0.5, random_state=None,\n",
       "       shuffle=True, solver='adam', tol=0.0001, validation_fraction=0.1,\n",
       "       verbose=True, warm_start=False)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp_play = MLPClassifier(activation='relu', hidden_layer_sizes=(100,100), alpha=0.01, verbose=True)\n",
    "mlp_play.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.83794844066528651"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp_play.score(X_test_scaled, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's try a different probability cut, not the one given by default to predict().\n",
    "# We choose the top 15%, but can optimize\n",
    "y_train_prob_play = mlp_play.predict_proba(X_train_scaled)[:, 1]\n",
    "y_test_prob_play = mlp_play.predict_proba(X_test_scaled)[:, 1]\n",
    "pcut_play = np.percentile(y_train_prob_scaled,85)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's look at event yields that pass our selection\n",
    "sigtrain_sel_play = weight_train.dot(np.multiply(y_train, y_train_prob_play > pcut_play))\n",
    "backtrain_sel_play = weight_train.dot(np.multiply(y_train == 0, y_train_prob_play > pcut_play))\n",
    "\n",
    "sigtest_sel_play = weight_test.dot(np.multiply(y_test, y_test_prob_play > pcut_play))\n",
    "backtest_sel_play = weight_test.dot(np.multiply(y_test == 0, y_test_prob_play > pcut_play))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corrected selected yields in training sample, signal = 285.721029386 , background = 6262.75114743\n",
      "Corrected selected yields in test sample, signal = 283.997485301 , background = 6650.1640173\n"
     ]
    }
   ],
   "source": [
    "# Now we need to correct the selected yields to be is if we used the full sample\n",
    "sigtrain_sel_play_corr = sigtrain_sel_play*sigall/sigtrain\n",
    "backtrain_sel_play_corr = backtrain_sel_play*backall/backtrain\n",
    "\n",
    "sigtest_sel_play_corr = sigtest_sel_play*sigall/sigtest\n",
    "backtest_sel_play_corr = backtest_sel_play*backall/backtest\n",
    "\n",
    "print(\"Corrected selected yields in training sample, signal =\", sigtrain_sel_play_corr, \", background =\",backtrain_sel_play_corr)\n",
    "print(\"Corrected selected yields in test sample, signal =\", sigtest_sel_play_corr, \", background =\",backtest_sel_play_corr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AMS of training sample 3.580676739279546\n",
      "AMS of test sample 3.45564003442452\n"
     ]
    }
   ],
   "source": [
    "print(\"AMS of training sample\", ams(sigtrain_sel_play_corr,backtrain_sel_play_corr))\n",
    "print(\"AMS of test sample\", ams(sigtest_sel_play_corr,backtest_sel_play_corr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problems (can do with either MLPClassifier or Keras):\n",
    "1. Vary the structure of the tree (number of hidden layers, number of neurons)\n",
    "1. Vary the activation. (In Keras can do it per layer, in MLPClassifier only for all)\n",
    "1. Vary the regularization. May have to do this as the structure changes.\n",
    "1. Try using derivied variables only or primary variables only.\n",
    "1. Missing data is represented by -999 before scaling. Is there a better value to use in the training?\n",
    "1. Try using the event weights to better match the background and signal shapes in the training. Note, though, that you should still treat background and signal separately; don't scale the signal down by the weight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
