{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Higgs Challenge Example using Neural Networks\n",
    "In this part we will look at the **[Higgs Boson ML Challenge](https://www.kaggle.com/c/Higgs-boson)** on Kaggle and attempt a solution using neural networks (NN). The data is available from **[CERN Open Data](http://opendata.cern.ch/record/328)**. More information about the data is available from the links, and in particular at **[Documentation](http://opendata.cern.ch/record/329/files/atlas-higgs-challenge-2014.pdf)**. The general idea is that we want to extract $H\\to\\tau\\tau$ signal from background. In particular, the selection requires one of the taus to decay into an electron or muon and two neutrinos, and the other into hadrons and a neutrino. The challenge is based on Monte Carlo events processed through the **[ATLAS detector](http://atlas.cern/)** simulation and reconstruction.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background on Neural Networks\n",
    "(based on lectures from **[ML Course on Coursera](https://www.coursera.org/learn/machine-learning)**)\n",
    "\n",
    "As we saw from the logistic regression yesterday, linear classifiers are often not the best at solving complicated problems. Neural networks introduce nonlinearity. They were originally designed to mimic the brain, and were popular in the 80s and early 90s. Recently they have become popular again, especially as deep neural networks DNNs, including convolutional NNs (CNN), recurrent NNs (RNN), etc. Those are beyond the scope of this class, but we will introduce the basics of NNs.\n",
    "\n",
    "Below is a diagram of a simple NN:\n",
    "![NNFig](https://upload.wikimedia.org/wikipedia/commons/4/46/Colored_neural_network.svg)\n",
    "It is made up of \"neurons\" that get a number of inputs, processes them, and sends the output to other neurons. Mathematically, one can represent the a neuron's \"activation\" $a = g\\left(\\theta^Tx\\right)$, where $x$ are the inputs (a vector), and $\\theta$ are the parameters (weights) of the model (also a vector), and $g$ is the activation fuction. For example, if we use a logistic function as the activation function, we can have $g\\left(\\theta^Tx\\right) = \\frac{1}{1+\\mathrm{exp}\\left(-\\theta^Tx\\right)}$, or if a Rectified Linear Unit (ReLU), $g\\left(\\theta^Tx\\right) = \\mathrm{max}\\left(0, \\theta^Tx\\right)$. The NN above has an input layer (layer 1), a hidden layer (layer 2), and an output layer (layer 3). One can have more hidden layers. Let's label the activations of layer 2 as $a_i^{(2)} = g\\left(\\theta_i^{(1)T}x\\right)$, where $i$ is the index of the individual neurons. Note that the subscript of the $\\theta$ is (1). That is because these are the weights going from layer 1 to 2. Putting together all the individual weight vectors together forms a matrix $\\Theta^{(1)}$.\n",
    "\n",
    "Using matrix notation, we can define $z^{(j)} = \\Theta^{(j-1)}a^{(j-1)}$ and then $a^{(j)} = g(z^{(j)})$. Thus evaluating the NN is a series of matrix multiplications followed by activation functions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cost function of a NN is similar to what we have for logistic regression, modified to take into account possible multiple outputs, and with more complicated regularization. In order to train the NN, we have to determine the weight matrix $\\Theta$ that minimizes the cost function. Backpropagation is the method used to do that. It calculates the partial derivatives \"errors\" for each $z_i^{(j)}$ by propagating the errors backwards. Usually something like (stochastic) gradient descent is used to solve the problem. For more details on backprorpagation, look, for example, at the **[ML course](https://www.coursera.org/learn/machine-learning)** mentioned above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's start trying to apply a NN to the Higgs Challenge data. We will start using Scikit Learn, and then try **[Keras](https://keras.io/)**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the usual setup: \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load training data\n",
    "df = pd.read_csv('data/atlas-higgs-challenge-2014-v2.csv.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EventId</th>\n",
       "      <th>DER_mass_MMC</th>\n",
       "      <th>DER_mass_transverse_met_lep</th>\n",
       "      <th>DER_mass_vis</th>\n",
       "      <th>DER_pt_h</th>\n",
       "      <th>DER_deltaeta_jet_jet</th>\n",
       "      <th>DER_mass_jet_jet</th>\n",
       "      <th>DER_prodeta_jet_jet</th>\n",
       "      <th>DER_deltar_tau_lep</th>\n",
       "      <th>DER_pt_tot</th>\n",
       "      <th>...</th>\n",
       "      <th>PRI_jet_leading_eta</th>\n",
       "      <th>PRI_jet_leading_phi</th>\n",
       "      <th>PRI_jet_subleading_pt</th>\n",
       "      <th>PRI_jet_subleading_eta</th>\n",
       "      <th>PRI_jet_subleading_phi</th>\n",
       "      <th>PRI_jet_all_pt</th>\n",
       "      <th>Weight</th>\n",
       "      <th>Label</th>\n",
       "      <th>KaggleSet</th>\n",
       "      <th>KaggleWeight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100000</td>\n",
       "      <td>138.470</td>\n",
       "      <td>51.655</td>\n",
       "      <td>97.827</td>\n",
       "      <td>27.980</td>\n",
       "      <td>0.91</td>\n",
       "      <td>124.711</td>\n",
       "      <td>2.666</td>\n",
       "      <td>3.064</td>\n",
       "      <td>41.928</td>\n",
       "      <td>...</td>\n",
       "      <td>2.150</td>\n",
       "      <td>0.444</td>\n",
       "      <td>46.062</td>\n",
       "      <td>1.24</td>\n",
       "      <td>-2.475</td>\n",
       "      <td>113.497</td>\n",
       "      <td>0.000814</td>\n",
       "      <td>s</td>\n",
       "      <td>t</td>\n",
       "      <td>0.002653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100001</td>\n",
       "      <td>160.937</td>\n",
       "      <td>68.768</td>\n",
       "      <td>103.235</td>\n",
       "      <td>48.146</td>\n",
       "      <td>-999.00</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>3.473</td>\n",
       "      <td>2.078</td>\n",
       "      <td>...</td>\n",
       "      <td>0.725</td>\n",
       "      <td>1.158</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.00</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>46.226</td>\n",
       "      <td>0.681042</td>\n",
       "      <td>b</td>\n",
       "      <td>t</td>\n",
       "      <td>2.233584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100002</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>162.172</td>\n",
       "      <td>125.953</td>\n",
       "      <td>35.635</td>\n",
       "      <td>-999.00</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>3.148</td>\n",
       "      <td>9.336</td>\n",
       "      <td>...</td>\n",
       "      <td>2.053</td>\n",
       "      <td>-2.028</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.00</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>44.251</td>\n",
       "      <td>0.715742</td>\n",
       "      <td>b</td>\n",
       "      <td>t</td>\n",
       "      <td>2.347389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100003</td>\n",
       "      <td>143.905</td>\n",
       "      <td>81.417</td>\n",
       "      <td>80.943</td>\n",
       "      <td>0.414</td>\n",
       "      <td>-999.00</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>3.310</td>\n",
       "      <td>0.414</td>\n",
       "      <td>...</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.00</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>1.660654</td>\n",
       "      <td>b</td>\n",
       "      <td>t</td>\n",
       "      <td>5.446378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100004</td>\n",
       "      <td>175.864</td>\n",
       "      <td>16.915</td>\n",
       "      <td>134.805</td>\n",
       "      <td>16.405</td>\n",
       "      <td>-999.00</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>3.891</td>\n",
       "      <td>16.405</td>\n",
       "      <td>...</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.00</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.904263</td>\n",
       "      <td>b</td>\n",
       "      <td>t</td>\n",
       "      <td>6.245333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   EventId  DER_mass_MMC  DER_mass_transverse_met_lep  DER_mass_vis  DER_pt_h  \\\n",
       "0   100000       138.470                       51.655        97.827    27.980   \n",
       "1   100001       160.937                       68.768       103.235    48.146   \n",
       "2   100002      -999.000                      162.172       125.953    35.635   \n",
       "3   100003       143.905                       81.417        80.943     0.414   \n",
       "4   100004       175.864                       16.915       134.805    16.405   \n",
       "\n",
       "   DER_deltaeta_jet_jet  DER_mass_jet_jet  DER_prodeta_jet_jet  \\\n",
       "0                  0.91           124.711                2.666   \n",
       "1               -999.00          -999.000             -999.000   \n",
       "2               -999.00          -999.000             -999.000   \n",
       "3               -999.00          -999.000             -999.000   \n",
       "4               -999.00          -999.000             -999.000   \n",
       "\n",
       "   DER_deltar_tau_lep  DER_pt_tot      ...       PRI_jet_leading_eta  \\\n",
       "0               3.064      41.928      ...                     2.150   \n",
       "1               3.473       2.078      ...                     0.725   \n",
       "2               3.148       9.336      ...                     2.053   \n",
       "3               3.310       0.414      ...                  -999.000   \n",
       "4               3.891      16.405      ...                  -999.000   \n",
       "\n",
       "   PRI_jet_leading_phi  PRI_jet_subleading_pt  PRI_jet_subleading_eta  \\\n",
       "0                0.444                 46.062                    1.24   \n",
       "1                1.158               -999.000                 -999.00   \n",
       "2               -2.028               -999.000                 -999.00   \n",
       "3             -999.000               -999.000                 -999.00   \n",
       "4             -999.000               -999.000                 -999.00   \n",
       "\n",
       "   PRI_jet_subleading_phi  PRI_jet_all_pt    Weight  Label  KaggleSet  \\\n",
       "0                  -2.475         113.497  0.000814      s          t   \n",
       "1                -999.000          46.226  0.681042      b          t   \n",
       "2                -999.000          44.251  0.715742      b          t   \n",
       "3                -999.000          -0.000  1.660654      b          t   \n",
       "4                -999.000           0.000  1.904263      b          t   \n",
       "\n",
       "   KaggleWeight  \n",
       "0      0.002653  \n",
       "1      2.233584  \n",
       "2      2.347389  \n",
       "3      5.446378  \n",
       "4      6.245333  \n",
       "\n",
       "[5 rows x 35 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is more information about the variables in the documentation. The variables that start with **DER** are derived quantities, determined by the physicists performing the analysis as variables that discriminate signal from backround. On the other hand, those that start with **PRI** are considered to be primary variables, from which the derived variables are calculated. They themselves generally do not provide much discrimination, but one if the ideas suggested by deep networks is that they can determine the necessary features from the primary variables, potentially even finding variables that the physicists did not consider. *EventId* identifies the event but is not a \"feature.\" The *Weight* is the event weight so that the sum of weights of all signal events should produce the signal yield expected to be observed in 2012, and the sum of weights of all background events should produce the backgroudn yield. Note that the weight varies event to event, because different background and signal processes contribute to the background and signal sets. *Label* indicates if it is a signal or background event. Ignore the *Kaggle* variables--they are only used if you want to reproduce exactly what was used in the Challenge. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map y values to integers\n",
    "df['Label'] = df['Label'].map({'b':0, 's':1})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EventId</th>\n",
       "      <th>DER_mass_MMC</th>\n",
       "      <th>DER_mass_transverse_met_lep</th>\n",
       "      <th>DER_mass_vis</th>\n",
       "      <th>DER_pt_h</th>\n",
       "      <th>DER_deltaeta_jet_jet</th>\n",
       "      <th>DER_mass_jet_jet</th>\n",
       "      <th>DER_prodeta_jet_jet</th>\n",
       "      <th>DER_deltar_tau_lep</th>\n",
       "      <th>DER_pt_tot</th>\n",
       "      <th>...</th>\n",
       "      <th>PRI_jet_leading_eta</th>\n",
       "      <th>PRI_jet_leading_phi</th>\n",
       "      <th>PRI_jet_subleading_pt</th>\n",
       "      <th>PRI_jet_subleading_eta</th>\n",
       "      <th>PRI_jet_subleading_phi</th>\n",
       "      <th>PRI_jet_all_pt</th>\n",
       "      <th>Weight</th>\n",
       "      <th>Label</th>\n",
       "      <th>KaggleSet</th>\n",
       "      <th>KaggleWeight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100000</td>\n",
       "      <td>138.470</td>\n",
       "      <td>51.655</td>\n",
       "      <td>97.827</td>\n",
       "      <td>27.980</td>\n",
       "      <td>0.91</td>\n",
       "      <td>124.711</td>\n",
       "      <td>2.666</td>\n",
       "      <td>3.064</td>\n",
       "      <td>41.928</td>\n",
       "      <td>...</td>\n",
       "      <td>2.150</td>\n",
       "      <td>0.444</td>\n",
       "      <td>46.062</td>\n",
       "      <td>1.24</td>\n",
       "      <td>-2.475</td>\n",
       "      <td>113.497</td>\n",
       "      <td>0.000814</td>\n",
       "      <td>1</td>\n",
       "      <td>t</td>\n",
       "      <td>0.002653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100001</td>\n",
       "      <td>160.937</td>\n",
       "      <td>68.768</td>\n",
       "      <td>103.235</td>\n",
       "      <td>48.146</td>\n",
       "      <td>-999.00</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>3.473</td>\n",
       "      <td>2.078</td>\n",
       "      <td>...</td>\n",
       "      <td>0.725</td>\n",
       "      <td>1.158</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.00</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>46.226</td>\n",
       "      <td>0.681042</td>\n",
       "      <td>0</td>\n",
       "      <td>t</td>\n",
       "      <td>2.233584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100002</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>162.172</td>\n",
       "      <td>125.953</td>\n",
       "      <td>35.635</td>\n",
       "      <td>-999.00</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>3.148</td>\n",
       "      <td>9.336</td>\n",
       "      <td>...</td>\n",
       "      <td>2.053</td>\n",
       "      <td>-2.028</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.00</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>44.251</td>\n",
       "      <td>0.715742</td>\n",
       "      <td>0</td>\n",
       "      <td>t</td>\n",
       "      <td>2.347389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100003</td>\n",
       "      <td>143.905</td>\n",
       "      <td>81.417</td>\n",
       "      <td>80.943</td>\n",
       "      <td>0.414</td>\n",
       "      <td>-999.00</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>3.310</td>\n",
       "      <td>0.414</td>\n",
       "      <td>...</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.00</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>1.660654</td>\n",
       "      <td>0</td>\n",
       "      <td>t</td>\n",
       "      <td>5.446378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100004</td>\n",
       "      <td>175.864</td>\n",
       "      <td>16.915</td>\n",
       "      <td>134.805</td>\n",
       "      <td>16.405</td>\n",
       "      <td>-999.00</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>3.891</td>\n",
       "      <td>16.405</td>\n",
       "      <td>...</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.00</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.904263</td>\n",
       "      <td>0</td>\n",
       "      <td>t</td>\n",
       "      <td>6.245333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   EventId  DER_mass_MMC  DER_mass_transverse_met_lep  DER_mass_vis  DER_pt_h  \\\n",
       "0   100000       138.470                       51.655        97.827    27.980   \n",
       "1   100001       160.937                       68.768       103.235    48.146   \n",
       "2   100002      -999.000                      162.172       125.953    35.635   \n",
       "3   100003       143.905                       81.417        80.943     0.414   \n",
       "4   100004       175.864                       16.915       134.805    16.405   \n",
       "\n",
       "   DER_deltaeta_jet_jet  DER_mass_jet_jet  DER_prodeta_jet_jet  \\\n",
       "0                  0.91           124.711                2.666   \n",
       "1               -999.00          -999.000             -999.000   \n",
       "2               -999.00          -999.000             -999.000   \n",
       "3               -999.00          -999.000             -999.000   \n",
       "4               -999.00          -999.000             -999.000   \n",
       "\n",
       "   DER_deltar_tau_lep  DER_pt_tot      ...       PRI_jet_leading_eta  \\\n",
       "0               3.064      41.928      ...                     2.150   \n",
       "1               3.473       2.078      ...                     0.725   \n",
       "2               3.148       9.336      ...                     2.053   \n",
       "3               3.310       0.414      ...                  -999.000   \n",
       "4               3.891      16.405      ...                  -999.000   \n",
       "\n",
       "   PRI_jet_leading_phi  PRI_jet_subleading_pt  PRI_jet_subleading_eta  \\\n",
       "0                0.444                 46.062                    1.24   \n",
       "1                1.158               -999.000                 -999.00   \n",
       "2               -2.028               -999.000                 -999.00   \n",
       "3             -999.000               -999.000                 -999.00   \n",
       "4             -999.000               -999.000                 -999.00   \n",
       "\n",
       "   PRI_jet_subleading_phi  PRI_jet_all_pt    Weight  Label  KaggleSet  \\\n",
       "0                  -2.475         113.497  0.000814      1          t   \n",
       "1                -999.000          46.226  0.681042      0          t   \n",
       "2                -999.000          44.251  0.715742      0          t   \n",
       "3                -999.000          -0.000  1.660654      0          t   \n",
       "4                -999.000           0.000  1.904263      0          t   \n",
       "\n",
       "   KaggleWeight  \n",
       "0      0.002653  \n",
       "1      2.233584  \n",
       "2      2.347389  \n",
       "3      5.446378  \n",
       "4      6.245333  \n",
       "\n",
       "[5 rows x 35 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's create separate arrays\n",
    "eventID = df['EventId']\n",
    "X = df.loc[:,'DER_mass_MMC':'PRI_jet_all_pt']\n",
    "y = df['Label']\n",
    "weight = df['Weight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now split into testing and training samples\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test, eventID_train, event_ID_test, weight_train, weight_test = train_test_split(\n",
    "    X, y, eventID, weight, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let's first look at a NN in sklearn\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "mlp = MLPClassifier(verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.98270141\n",
      "Iteration 2, loss = 0.70650215\n",
      "Iteration 3, loss = 0.64800695\n",
      "Iteration 4, loss = 0.64291382\n",
      "Iteration 5, loss = 0.59437137\n",
      "Iteration 6, loss = 0.56616702\n",
      "Iteration 7, loss = 0.54513114\n",
      "Iteration 8, loss = 0.53357325\n",
      "Iteration 9, loss = 0.50208177\n",
      "Iteration 10, loss = 0.49216077\n",
      "Iteration 11, loss = 0.48369652\n",
      "Iteration 12, loss = 0.47536833\n",
      "Iteration 13, loss = 0.46379574\n",
      "Iteration 14, loss = 0.45551704\n",
      "Iteration 15, loss = 0.44735322\n",
      "Iteration 16, loss = 0.43738810\n",
      "Iteration 17, loss = 0.43277851\n",
      "Iteration 18, loss = 0.42866314\n",
      "Iteration 19, loss = 0.42387502\n",
      "Iteration 20, loss = 0.41924680\n",
      "Iteration 21, loss = 0.41725630\n",
      "Iteration 22, loss = 0.41229860\n",
      "Iteration 23, loss = 0.41072650\n",
      "Iteration 24, loss = 0.40875766\n",
      "Iteration 25, loss = 0.40678001\n",
      "Iteration 26, loss = 0.40414717\n",
      "Iteration 27, loss = 0.40426950\n",
      "Iteration 28, loss = 0.40120989\n",
      "Iteration 29, loss = 0.40133914\n",
      "Iteration 30, loss = 0.40021265\n",
      "Iteration 31, loss = 0.39812609\n",
      "Iteration 32, loss = 0.39789200\n",
      "Iteration 33, loss = 0.39812886\n",
      "Iteration 34, loss = 0.39775061\n",
      "Iteration 35, loss = 0.39705854\n",
      "Iteration 36, loss = 0.39548767\n",
      "Iteration 37, loss = 0.39584130\n",
      "Iteration 38, loss = 0.39647363\n",
      "Iteration 39, loss = 0.39580477\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
       "       nesterovs_momentum=True, power_t=0.5, random_state=None,\n",
       "       shuffle=True, solver='adam', tol=0.0001, validation_fraction=0.1,\n",
       "       verbose=True, warm_start=False)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# and train\n",
    "mlp.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.82248656576018719"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Kaggle competition used the approximate median segnificance (AMS), as defined below, to determine how good a solution was. The number 10, added to the background yield, is a regularization term to decrease the variance of the AMS.\n",
    "\n",
    "Note that if you do not use the full data set (i.e. you split into training and testing) you have to reweigh the inputs so that the subsample yield matches to the toal yield, which we will do below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute approximate median segnificance (AMS)\n",
    "\n",
    "def ams(s,b):\n",
    "    from math import sqrt,log\n",
    "    if b==0:\n",
    "        return 0\n",
    "\n",
    "    return sqrt(2*((s+b+10)*log(1+float(s)/(b+10))-s))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's try a different probability cut, not the one given by default to predict().\n",
    "# We choose the top 15%, but can optimize\n",
    "y_train_prob = mlp.predict_proba(X_train)[:, 1]\n",
    "y_test_prob = mlp.predict_proba(X_test)[:, 1]\n",
    "pcut = np.percentile(y_train_prob,85)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the probability to the original data frame\n",
    "df['Prob']=mlp.predict_proba(X)[:, 1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x10bd93080>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10d2e15f8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "kwargs = dict(histtype='stepfilled', alpha=0.3, normed=True, bins=40)\n",
    "\n",
    "df[df.Label==0].Prob.hist(label='Background',**kwargs)\n",
    "df[df.Label==1].Prob.hist(label='Signal',**kwargs)\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's calculate the total weights (yields)\n",
    "sigall = weight.dot(y)\n",
    "backall = weight.dot(y == 0)\n",
    "\n",
    "# The training weights\n",
    "sigtrain = weight_train.dot(y_train)\n",
    "backtrain = weight_train.dot(y_train == 0)\n",
    "\n",
    "# The training weights\n",
    "sigtest = weight_test.dot(y_test)\n",
    "backtest = weight_test.dot(y_test == 0)\n",
    "\n",
    "# aside:  these can also be done by looping instead of using a dot product\n",
    "#  (Usually vectorized operations are faster for interpreted code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's look at event yields that pass our selection\n",
    "sigtrain_sel = weight_train.dot(np.multiply(y_train, y_train_prob > pcut))\n",
    "backtrain_sel = weight_train.dot(np.multiply(y_train == 0, y_train_prob > pcut))\n",
    "\n",
    "sigtest_sel = weight_test.dot(np.multiply(y_test, y_test_prob > pcut))\n",
    "backtest_sel = weight_test.dot(np.multiply(y_test == 0, y_test_prob > pcut))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corrected selected yields in training sample, signal = 219.347205361 , background = 4750.45596371\n",
      "Corrected selected yields in test sample, signal = 220.018857952 , background = 4931.57036877\n"
     ]
    }
   ],
   "source": [
    "# Now we need to correct the selected yields to be is if we used the full sample\n",
    "sigtrain_sel_corr = sigtrain_sel*sigall/sigtrain\n",
    "backtrain_sel_corr = backtrain_sel*backall/backtrain\n",
    "\n",
    "sigtest_sel_corr = sigtest_sel*sigall/sigtest\n",
    "backtest_sel_corr = backtest_sel*backall/backtest\n",
    "\n",
    "print(\"Corrected selected yields in training sample, signal =\", sigtrain_sel_corr, \", background =\",backtrain_sel_corr)\n",
    "print(\"Corrected selected yields in test sample, signal =\", sigtest_sel_corr, \", background =\",backtest_sel_corr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AMS of training sample 3.1551696906404554\n",
      "AMS of test sample 3.1070727666799316\n"
     ]
    }
   ],
   "source": [
    "print(\"AMS of training sample\", ams(sigtrain_sel_corr,backtrain_sel_corr))\n",
    "print(\"AMS of test sample\", ams(sigtest_sel_corr,backtest_sel_corr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How did we do? Worse than the BDT from yesterday.\n",
    "![Comparison with submissions](data/tr150908_davidRousseau_TMVAFuture_HiggsML.001.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural networks are quite sensitive to feature scaling, so let's try to scale the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import RobustScaler\n",
    "scaler = RobustScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.40210580\n",
      "Iteration 2, loss = 0.37627343\n",
      "Iteration 3, loss = 0.37072309\n",
      "Iteration 4, loss = 0.36791575\n",
      "Iteration 5, loss = 0.36648127\n",
      "Iteration 6, loss = 0.36504797\n",
      "Iteration 7, loss = 0.36432405\n",
      "Iteration 8, loss = 0.36365296\n",
      "Iteration 9, loss = 0.36313779\n",
      "Iteration 10, loss = 0.36277819\n",
      "Iteration 11, loss = 0.36238728\n",
      "Iteration 12, loss = 0.36195794\n",
      "Iteration 13, loss = 0.36172647\n",
      "Iteration 14, loss = 0.36146493\n",
      "Iteration 15, loss = 0.36118396\n",
      "Iteration 16, loss = 0.36097046\n",
      "Iteration 17, loss = 0.36067766\n",
      "Iteration 18, loss = 0.36041217\n",
      "Iteration 19, loss = 0.36034322\n",
      "Iteration 20, loss = 0.36010304\n",
      "Iteration 21, loss = 0.36000407\n",
      "Iteration 22, loss = 0.35971477\n",
      "Iteration 23, loss = 0.35965408\n",
      "Iteration 24, loss = 0.35948312\n",
      "Iteration 25, loss = 0.35929238\n",
      "Iteration 26, loss = 0.35916584\n",
      "Iteration 27, loss = 0.35916680\n",
      "Iteration 28, loss = 0.35900119\n",
      "Iteration 29, loss = 0.35886586\n",
      "Iteration 30, loss = 0.35885280\n",
      "Iteration 31, loss = 0.35868764\n",
      "Iteration 32, loss = 0.35858282\n",
      "Iteration 33, loss = 0.35847673\n",
      "Iteration 34, loss = 0.35844422\n",
      "Iteration 35, loss = 0.35849305\n",
      "Iteration 36, loss = 0.35834322\n",
      "Iteration 37, loss = 0.35821105\n",
      "Iteration 38, loss = 0.35805059\n",
      "Iteration 39, loss = 0.35800586\n",
      "Iteration 40, loss = 0.35794888\n",
      "Iteration 41, loss = 0.35794874\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
       "       nesterovs_momentum=True, power_t=0.5, random_state=None,\n",
       "       shuffle=True, solver='adam', tol=0.0001, validation_fraction=0.1,\n",
       "       verbose=True, warm_start=False)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# and train a new network\n",
    "mlp_scaled = MLPClassifier(verbose=True)\n",
    "mlp_scaled.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.83703369022179919"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp_scaled.score(X_test_scaled, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's try a different probability cut, not the one given by default to predict().\n",
    "# We choose the top 15%, but can optimize\n",
    "y_train_prob_scaled = mlp_scaled.predict_proba(X_train_scaled)[:, 1]\n",
    "y_test_prob_scaled = mlp_scaled.predict_proba(X_test_scaled)[:, 1]\n",
    "pcut_scaled = np.percentile(y_train_prob_scaled,85)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's look at event yields that pass our selection\n",
    "sigtrain_sel_scaled = weight_train.dot(np.multiply(y_train, y_train_prob_scaled > pcut_scaled))\n",
    "backtrain_sel_scaled = weight_train.dot(np.multiply(y_train == 0, y_train_prob_scaled > pcut_scaled))\n",
    "\n",
    "sigtest_sel_scaled = weight_test.dot(np.multiply(y_test, y_test_prob_scaled > pcut_scaled))\n",
    "backtest_sel_scaled = weight_test.dot(np.multiply(y_test == 0, y_test_prob_scaled > pcut_scaled))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corrected selected yields in training sample, signal = 246.871516281 , background = 4967.47427104\n",
      "Corrected selected yields in test sample, signal = 245.593074532 , background = 5273.15790218\n"
     ]
    }
   ],
   "source": [
    "# Now we need to correct the selected yields to be is if we used the full sample\n",
    "sigtrain_sel_scaled_corr = sigtrain_sel_scaled*sigall/sigtrain\n",
    "backtrain_sel_scaled_corr = backtrain_sel_scaled*backall/backtrain\n",
    "\n",
    "sigtest_sel_scaled_corr = sigtest_sel_scaled*sigall/sigtest\n",
    "backtest_sel_scaled_corr = backtest_sel_scaled*backall/backtest\n",
    "\n",
    "print(\"Corrected selected yields in training sample, signal =\", sigtrain_sel_scaled_corr, \", background =\",backtrain_sel_scaled_corr)\n",
    "print(\"Corrected selected yields in test sample, signal =\", sigtest_sel_scaled_corr, \", background =\",backtest_sel_scaled_corr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AMS of training sample 3.4708381281557745\n",
      "AMS of test sample 3.353169310835285\n"
     ]
    }
   ],
   "source": [
    "print(\"AMS of training sample\", ams(sigtrain_sel_scaled_corr,backtrain_sel_scaled_corr))\n",
    "print(\"AMS of test sample\", ams(sigtest_sel_scaled_corr,backtest_sel_scaled_corr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We improved somewhat.\n",
    "\n",
    "SciKit Learn has simple NNs, but if you want to do deep NNs, or train on GPUs, you probalby want to use something like Keras instead. Let's try to create a simple NN using Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1337)  # for reproducibility\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the model\n",
    "model = Sequential()\n",
    "model.add(Dense(100, activation='relu', input_shape=(30,)))\n",
    "model.add(Dense(1, activation='sigmoid'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "548219/548219 [==============================] - 11s 20us/step - loss: 0.3655 - acc: 0.8364\n",
      "Epoch 2/20\n",
      "548219/548219 [==============================] - 11s 19us/step - loss: 0.3642 - acc: 0.8370\n",
      "Epoch 3/20\n",
      "548219/548219 [==============================] - 10s 19us/step - loss: 0.3632 - acc: 0.8372\n",
      "Epoch 4/20\n",
      "548219/548219 [==============================] - 12s 22us/step - loss: 0.3626 - acc: 0.8377\n",
      "Epoch 5/20\n",
      "548219/548219 [==============================] - 11s 20us/step - loss: 0.3619 - acc: 0.8381\n",
      "Epoch 6/20\n",
      "548219/548219 [==============================] - 11s 20us/step - loss: 0.3614 - acc: 0.8381\n",
      "Epoch 7/20\n",
      "548219/548219 [==============================] - 11s 20us/step - loss: 0.3612 - acc: 0.8385\n",
      "Epoch 8/20\n",
      "548219/548219 [==============================] - 10s 19us/step - loss: 0.3608 - acc: 0.8385\n",
      "Epoch 9/20\n",
      "548219/548219 [==============================] - 11s 20us/step - loss: 0.3604 - acc: 0.8387\n",
      "Epoch 10/20\n",
      "548219/548219 [==============================] - 10s 19us/step - loss: 0.3603 - acc: 0.8387\n",
      "Epoch 11/20\n",
      "548219/548219 [==============================] - 12s 22us/step - loss: 0.3600 - acc: 0.8388\n",
      "Epoch 12/20\n",
      "548219/548219 [==============================] - 11s 20us/step - loss: 0.3598 - acc: 0.8389\n",
      "Epoch 13/20\n",
      "548219/548219 [==============================] - 12s 22us/step - loss: 0.3595 - acc: 0.8392\n",
      "Epoch 14/20\n",
      "548219/548219 [==============================] - 12s 22us/step - loss: 0.3595 - acc: 0.8389\n",
      "Epoch 15/20\n",
      "548219/548219 [==============================] - 12s 21us/step - loss: 0.3592 - acc: 0.8393\n",
      "Epoch 16/20\n",
      "548219/548219 [==============================] - 12s 22us/step - loss: 0.3590 - acc: 0.8393\n",
      "Epoch 17/20\n",
      "548219/548219 [==============================] - 11s 20us/step - loss: 0.3590 - acc: 0.8395\n",
      "Epoch 18/20\n",
      "548219/548219 [==============================] - 11s 19us/step - loss: 0.3588 - acc: 0.8393\n",
      "Epoch 19/20\n",
      "548219/548219 [==============================] - 14s 25us/step - loss: 0.3586 - acc: 0.8394\n",
      "Epoch 20/20\n",
      "548219/548219 [==============================] - 8s 14us/step - loss: 0.3585 - acc: 0.8397\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a4b140b70>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train_scaled, y_train, epochs=20, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's try a different probability cut, not the one given by default to predict().\n",
    "# We choose the top 15%, but can optimize\n",
    "y_train_prob_keras = model.predict(X_train_scaled)[:, 0]\n",
    "y_test_prob_keras = model.predict(X_test_scaled)[:, 0]\n",
    "pcut_keras = np.percentile(y_train_prob_keras,85)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's look at event yields that pass our selection\n",
    "sigtrain_sel_keras = weight_train.dot(np.multiply(y_train, y_train_prob_keras > pcut_keras))\n",
    "backtrain_sel_keras = weight_train.dot(np.multiply(y_train == 0, y_train_prob_keras > pcut_keras))\n",
    "\n",
    "sigtest_sel_keras = weight_test.dot(np.multiply(y_test, y_test_prob_keras > pcut_keras))\n",
    "backtest_sel_keras = weight_test.dot(np.multiply(y_test == 0, y_test_prob_keras > pcut_keras))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corrected selected yields in training sample, signal = 233.499170452 , background = 4203.34711742\n",
      "Corrected selected yields in test sample, signal = 231.313389116 , background = 4486.01490432\n"
     ]
    }
   ],
   "source": [
    "# Now we need to correct the selected yields to be is if we used the full sample\n",
    "sigtrain_sel_keras_corr = sigtrain_sel_keras*sigall/sigtrain\n",
    "backtrain_sel_keras_corr = backtrain_sel_keras*backall/backtrain\n",
    "\n",
    "sigtest_sel_keras_corr = sigtest_sel_keras*sigall/sigtest\n",
    "backtest_sel_keras_corr = backtest_sel_keras*backall/backtest\n",
    "\n",
    "print(\"Corrected selected yields in training sample, signal =\", sigtrain_sel_keras_corr, \", background =\",backtrain_sel_keras_corr)\n",
    "print(\"Corrected selected yields in test sample, signal =\", sigtest_sel_keras_corr, \", background =\",backtest_sel_keras_corr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AMS of training sample 3.564777581616436\n",
      "AMS of test sample 3.4207801822251915\n"
     ]
    }
   ],
   "source": [
    "print(\"AMS of training sample\", ams(sigtrain_sel_keras_corr,backtrain_sel_keras_corr))\n",
    "print(\"AMS of test sample\", ams(sigtest_sel_keras_corr,backtest_sel_keras_corr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We only made a single layer NN in Keras. However, you can easily change the structure of the network. As an assignment, try adding an extra hidden layer and changing the number of neurons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
